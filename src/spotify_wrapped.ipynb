{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l_RuSAE45163",
        "ESdtsht1EZad",
        "zBHj6x3gEhDQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoYVPXsfMEUu",
        "outputId": "dbe645ca-839b-437e-9b88-a3c304e9fc43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting faker\n",
            "  Downloading Faker-24.9.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Collecting fastavro\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.5/246.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting TDigest\n",
            "  Downloading tdigest-0.5.2.2-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Collecting accumulation-tree (from TDigest)\n",
            "  Downloading accumulation_tree-0.6.2.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyudorandom (from TDigest)\n",
            "  Downloading pyudorandom-1.0.0.tar.gz (1.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Building wheels for collected packages: accumulation-tree, pyudorandom\n",
            "  Building wheel for accumulation-tree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for accumulation-tree: filename=accumulation_tree-0.6.2-cp310-cp310-linux_x86_64.whl size=355229 sha256=805c362c1069ca30536f3b45d97108451a54e608863fbbf690db5df91d24c0c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/86/37/6b08c6765cc44d4482b5f7c088de741a24951e46ffc3731f80\n",
            "  Building wheel for pyudorandom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyudorandom: filename=pyudorandom-1.0.0-py3-none-any.whl size=2190 sha256=c11d0dc1937f3f8b953173fd7ff1be5bc08d20ba104b2616a9b1877070bfef91\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/38/93/20b648196ea752c07f107de4d07748b927989bef8e6effc1ef\n",
            "Successfully built accumulation-tree pyudorandom\n",
            "Installing collected packages: pyudorandom, kafka-python, accumulation-tree, TDigest, pyngrok, fastavro, faker\n",
            "Successfully installed TDigest-0.5.2.2 accumulation-tree-0.6.2 faker-24.9.0 fastavro-1.9.4 kafka-python-2.0.2 pyngrok-7.1.6 pyudorandom-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas faker pyngrok fastavro kafka-python TDigest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQqk5I6qJN7-",
        "outputId": "26be2340-7a9d-4d94-e44b-83c1740b16ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile environment.sh\n",
        "#!/usr/bin/bash\n",
        "export KAFKA_BINARY_VERSION='3.7.0'\n",
        "export SCALA_BINARY_VERSION='2.13'\n",
        "export KAFKA_BINARY_VERSION=$KAFKA_BINARY_VERSION\n",
        "export SCALA_BINARY_VERSION=$SCALA_BINARY_VERSION\n",
        "export PATH=$PATH:$PWD/kafka_$SCALA_BINARY_VERSION-$KAFKA_BINARY_VERSION/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IizICQPMIQ9",
        "outputId": "16f1ea86-a664-41d3-da2a-c8228a584340"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing environment.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kafka_setup.sh\n",
        "\n",
        "source ./environment.sh\n",
        "echo kafka_$SCALA_BINARY_VERSION-$KAFKA_BINARY_VERSION\n",
        "echo $PATH\n",
        "\n",
        "# Java Setup\n",
        "wget -O- https://apt.corretto.aws/corretto.key | sudo apt-key add -\n",
        "sudo add-apt-repository 'deb https://apt.corretto.aws stable main' -y\n",
        "sudo apt-get -y update; sudo apt-get install -y java-11-amazon-corretto-jdk\n",
        "\n",
        "# Kafka Setup\n",
        "wget https://downloads.apache.org/kafka/${KAFKA_BINARY_VERSION}/kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}.tgz\n",
        "tar xzf kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}.tgz\n",
        "\n",
        "UUID=$(./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/bin/kafka-storage.sh random-uuid)\n",
        "echo \"export UUID=$UUID\" >> ./environment.sh\n",
        "cat environment.sh\n",
        "\n",
        "# Start Kafka Broker\n",
        "\n",
        "echo kafka_$SCALA_BINARY_VERSION-$KAFKA_BINARY_VERSION\n",
        "\n",
        "# offsets.retention.minutes determines how long Kafka retains the commit offsets for consumer groups.\n",
        "echo \"offsets.retention.minutes=300\" >> ./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/config/kraft/server.properties\n",
        "\n",
        "./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/bin/kafka-storage.sh format -t ${UUID} -c ./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/config/kraft/server.properties\n",
        "nohup ./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/bin/kafka-server-start.sh ./kafka_${SCALA_BINARY_VERSION}-${KAFKA_BINARY_VERSION}/config/kraft/server.properties > kafka_server.log &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_ZE-CKLMIwa",
        "outputId": "96d98fc7-0dd4-450b-8c85-9b6bc7bb1419"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kafka_setup.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source kafka_setup.sh\n",
        "sleep 10\n",
        "tail -20 kafka_server.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0qmAm38MPg2",
        "outputId": "fd86eff4-74d0-42de-9be5-ebf623b7d83e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kafka_2.13-3.7.0\n",
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/content/kafka_2.13-3.7.0/bin\n",
            "--2024-04-16 01:44:37--  https://apt.corretto.aws/corretto.key\n",
            "Resolving apt.corretto.aws (apt.corretto.aws)... 52.84.18.14, 52.84.18.105, 52.84.18.24, ...\n",
            "Connecting to apt.corretto.aws (apt.corretto.aws)|52.84.18.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1695 (1.7K) [binary/octet-stream]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-16 01:44:37 (835 MB/s) - written to stdout [1695/1695]\n",
            "\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Repository: 'deb https://apt.corretto.aws stable main'\n",
            "Description:\n",
            "Archive for codename: stable components: main\n",
            "More info: https://apt.corretto.aws\n",
            "Adding repository.\n",
            "Adding deb entry to /etc/apt/sources.list.d/archive_uri-https_apt_corretto_aws-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/archive_uri-https_apt_corretto_aws-jammy.list\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://apt.corretto.aws stable InRelease [10.7 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://apt.corretto.aws stable/main amd64 Packages [17.1 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [808 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,691 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,974 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,375 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Fetched 7,238 kB in 4s (1,832 kB/s)\n",
            "Reading package lists... Done\n",
            "W: https://apt.corretto.aws/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 https://apt.corretto.aws stable InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: https://apt.corretto.aws/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  java-11-amazon-corretto-jdk\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 195 MB of archives.\n",
            "After this operation, 327 MB of additional disk space will be used.\n",
            "Get:1 https://apt.corretto.aws stable/main amd64 java-11-amazon-corretto-jdk amd64 1:11.0.22.7-1 [195 MB]\n",
            "Fetched 195 MB in 4s (47.8 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package java-11-amazon-corretto-jdk:amd64.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../java-11-amazon-corretto-jdk_1%3a11.0.22.7-1_amd64.deb ...\n",
            "Unpacking java-11-amazon-corretto-jdk:amd64 (1:11.0.22.7-1) ...\n",
            "Setting up java-11-amazon-corretto-jdk:amd64 (1:11.0.22.7-1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/java to provide /usr/bin/java (java) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-amazon-corretto/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
            "--2024-04-16 01:45:10--  https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 119028138 (114M) [application/x-gzip]\n",
            "Saving to: ‘kafka_2.13-3.7.0.tgz’\n",
            "\n",
            "kafka_2.13-3.7.0.tg 100%[===================>] 113.51M  23.2MB/s    in 5.7s    \n",
            "\n",
            "2024-04-16 01:45:16 (19.8 MB/s) - ‘kafka_2.13-3.7.0.tgz’ saved [119028138/119028138]\n",
            "\n",
            "#!/usr/bin/bash\n",
            "export KAFKA_BINARY_VERSION='3.7.0'\n",
            "export SCALA_BINARY_VERSION='2.13'\n",
            "export KAFKA_BINARY_VERSION=$KAFKA_BINARY_VERSION\n",
            "export SCALA_BINARY_VERSION=$SCALA_BINARY_VERSION\n",
            "export PATH=$PATH:$PWD/kafka_$SCALA_BINARY_VERSION-$KAFKA_BINARY_VERSION/bin\n",
            "export UUID=Qs3LFeNpTv-FgHeqlN4zLw\n",
            "kafka_2.13-3.7.0\n",
            "metaPropertiesEnsemble=MetaPropertiesEnsemble(metadataLogDir=Optional.empty, dirs={/tmp/kraft-combined-logs: EMPTY})\n",
            "Formatting /tmp/kraft-combined-logs with metadata.version 3.7-IV4.\n",
            "nohup: redirecting stderr to stdout\n",
            "\tzookeeper.ssl.truststore.password = null\n",
            "\tzookeeper.ssl.truststore.type = null\n",
            " (kafka.server.KafkaConfig)\n",
            "[2024-04-16 01:45:27,732] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 8 (kafka.server.BrokerLifecycleManager)\n",
            "[2024-04-16 01:45:27,735] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)\n",
            "[2024-04-16 01:45:27,736] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)\n",
            "[2024-04-16 01:45:27,803] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)\n",
            "[2024-04-16 01:45:27,804] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)\n",
            "[2024-04-16 01:45:27,805] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)\n",
            "[2024-04-16 01:45:27,806] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)\n",
            "[2024-04-16 01:45:27,806] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n",
            "[2024-04-16 01:45:27,811] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\n",
            "[2024-04-16 01:45:27,811] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)\n",
            "[2024-04-16 01:45:27,812] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\n",
            "[2024-04-16 01:45:27,812] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)\n",
            "[2024-04-16 01:45:27,812] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)\n",
            "[2024-04-16 01:45:27,813] INFO Kafka version: 3.7.0 (org.apache.kafka.common.utils.AppInfoParser)\n",
            "[2024-04-16 01:45:27,813] INFO Kafka commitId: 2ae524ed625438c5 (org.apache.kafka.common.utils.AppInfoParser)\n",
            "[2024-04-16 01:45:27,813] INFO Kafka startTimeMs: 1713231927812 (org.apache.kafka.common.utils.AppInfoParser)\n",
            "[2024-04-16 01:45:27,815] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "source ./environment.sh\n",
        "\n",
        "kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --topic spotify --create --partitions 10 --replication-factor 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMRVo-3iMQ2c",
        "outputId": "6e65e81f-eeb8-4766-9f7e-5de337220341"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic spotify.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile check_kafka_consumers.sh\n",
        "#!/usr/bin/env bash\n",
        "source ./environment.sh\n",
        "\n",
        "echo \"Active Consumer Groups\"\n",
        "while true\n",
        "do\n",
        "date\n",
        "kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --all-groups\n",
        "sleep 1\n",
        "done\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYJX1ZAPMSdb",
        "outputId": "7caba024-ec55-4909-a0e9-fe054df4e192"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing check_kafka_consumers.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "chmod +x ./check_kafka_consumers.sh\n",
        "nohup ./check_kafka_consumers.sh > kafka_consumers.log &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55O9m0g6MT7i",
        "outputId": "c55ff021-45e9-45e3-d53f-94d630b2ce9c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python drive/MyDrive/stream/src/data_generator.py > avro_producer.log &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAIqSwEeMV0f",
        "outputId": "f4aac1f9-f50d-4eb6-db01-2aa57752ca7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sleep 5\n",
        "!tail -20 avro_producer.log"
      ],
      "metadata": {
        "id": "hXD8QK2wMXoa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile avro_consumer.py\n",
        "\n",
        "from kafka import KafkaConsumer\n",
        "import fastavro\n",
        "import io\n",
        "\n",
        "# Define the schema for the Spotify records\n",
        "schema = {\n",
        "    \"doc\": \"Spotify Wrapped Data Feed - User Event\",\n",
        "    \"name\": \"UserEvent\",\n",
        "    \"namespace\": \"com.spotify.wrapped\",\n",
        "    \"type\": \"record\",\n",
        "    \"fields\": [\n",
        "        {\"name\": \"id\", \"type\": \"long\"},\n",
        "        {\"name\": \"timestamp\", \"type\": \"string\", \"logicalType\": \"timestamp-millis\"},\n",
        "        {\"name\": \"track_id\", \"type\": \"string\"},\n",
        "        {\"name\": \"user_id\", \"type\": \"long\"},\n",
        "        {\"name\": \"listening_time\", \"type\": \"long\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Parse the schema\n",
        "parsed_schema = fastavro.parse_schema(schema)\n",
        "\n",
        "# Create a Kafka consumer with value deserializer\n",
        "def deserialize(message):\n",
        "    schemaless_bytes_reader = io.BytesIO(message)\n",
        "    try:\n",
        "        record = fastavro.schemaless_reader(schemaless_bytes_reader, parsed_schema)\n",
        "        return record\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        return None  # Return None for failed deserialization\n",
        "\n",
        "# Create a Kafka consumer with value deserializer\n",
        "consumer = KafkaConsumer(\n",
        "    'spotify',  # Adjust topic name as needed\n",
        "    bootstrap_servers=['localhost:9092'],  # Adjust bootstrap servers if needed\n",
        "    auto_offset_reset='earliest',\n",
        "    enable_auto_commit=True,\n",
        "    group_id='Python_AVRO_Consumer',  # Adjust consumer group ID if needed\n",
        "    value_deserializer=deserialize\n",
        ")\n",
        "\n",
        "# Consume messages from the topic and print them\n",
        "for message in consumer:\n",
        "    print(\"=\" * 10)\n",
        "    print(message.value)\n",
        "print('hi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2CyC99TMZie",
        "outputId": "8a36e48d-c531-411e-d88c-bbc24b516147"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing avro_consumer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python avro_consumer.py > avro_consumer.log &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSIPd3KUMcVv",
        "outputId": "ccb40aa2-0cc6-4681-f922-7fee36d418c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sleep 5\n",
        "!tail -20 avro_consumer.log"
      ],
      "metadata": {
        "id": "zBhqHODLMgYr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef |grep avro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kox36LS3Mh_i",
        "outputId": "f2e5380b-1ba4-4b19-da99-a7941dfd2585"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        5547       1  5 01:45 ?        00:00:00 python3 avro_consumer.py\n",
            "root        5579     221  0 01:45 ?        00:00:00 /bin/bash -c ps -ef |grep avro\n",
            "root        5581    5579  0 01:45 ?        00:00:00 grep avro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "source ./environment.sh\n",
        "kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPT9AsAWMiUa",
        "outputId": "15a25a03-687b-4c12-f234-f39735265e37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python_AVRO_Consumer\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "source ./environment.sh\n",
        "kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group Python_AVRO_Consumer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qehKqCL9Mvq7",
        "outputId": "145757ae-da2a-491a-b817-30911234da4f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GROUP                TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                             HOST            CLIENT-ID\n",
            "Python_AVRO_Consumer spotify         1          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         9          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         6          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         8          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         2          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         4          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         5          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         7          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         0          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         3          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -20 kafka_consumers.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd7mh3zCMv1B",
        "outputId": "6f896928-67ee-4eda-9e18-ef4becd2d398"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python_AVRO_Consumer spotify         2          -               0               -               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         4          -               0               -               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         5          -               0               -               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         7          -               0               -               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         0          -               0               -               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         3          -               0               -               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Tue Apr 16 01:45:50 AM UTC 2024\n",
            "\n",
            "GROUP                TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                             HOST            CLIENT-ID\n",
            "Python_AVRO_Consumer spotify         1          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         9          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         6          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         8          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         2          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         4          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         5          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         7          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         0          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         3          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Tue Apr 16 01:46:02 AM UTC 2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "source ./environment.sh\n",
        "kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F7Wi6WRMv8D",
        "outputId": "5c5528c8-287c-43fa-bec7-8b43fd92af5a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GROUP                TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                             HOST            CLIENT-ID\n",
            "Python_AVRO_Consumer spotify         1          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         9          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         6          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         8          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         2          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         4          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         5          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         7          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         0          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n",
            "Python_AVRO_Consumer spotify         3          0               0               0               kafka-python-2.0.2-aa379e5e-270b-44d0-8ff6-0be07f7b7be0 /127.0.0.1      kafka-python-2.0.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_release='spark-3.5.1'\n",
        "hadoop_version='hadoop3'\n",
        "\n",
        "import os, time\n",
        "start=time.time()\n",
        "os.environ['SPARK_RELEASE']=spark_release\n",
        "os.environ['HADOOP_VERSION']=hadoop_version\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_release}-bin-{hadoop_version}\""
      ],
      "metadata": {
        "id": "K0eTmwetMwEl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run below commands in google colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # install Java8\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # download spark-3.3.X\n",
        "!tar xf ${SPARK_RELEASE}-bin-${HADOOP_VERSION}.tgz # unzip it\n",
        "\n",
        "!pip install -q findspark # install findspark\n",
        "# findspark find your Spark Distribution and sets necessary environment variables\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiFutn2HM47x",
        "outputId": "6cd5fa13-a9d4-45a6-a280-47c80a89db4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kafka_brokers=\"127.0.0.1:9092\" # Can be a comma-separated list of brokers\n",
        "topic_name=\"spotify\"\n",
        "\n",
        "# Define the AVRO schema as a string\n",
        "schema = \"\"\"\n",
        "{\n",
        "        \"doc\": \"Spotify Wrapped Data Feed - User Event\",\n",
        "        \"name\": \"UserEvent\",\n",
        "        \"namespace\": \"com.spotify.wrapped\",\n",
        "        \"type\": \"record\",\n",
        "        \"fields\": [\n",
        "            {\"name\": \"id\", \"type\": \"long\"},\n",
        "            {\"name\": \"timestamp\", \"type\": \"string\",\n",
        "                \"logicalType\": \"timestamp-millis\"},\n",
        "            {\"name\": \"track_id\", \"type\": \"string\"},\n",
        "            {\"name\": \"user_id\", \"type\": \"long\"},\n",
        "            {\"name\": \"listening_time\", \"type\":\"long\"}\n",
        "        ]\n",
        "    }\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zDLy-2ARM5CS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"StreamingAVROFromKafka\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0') \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "IqrOVMGPM5Jw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kafka Configuration for reading from Kafka/Event Hub\n",
        "# Kafka source will create a unique group id for each query automatically. The user can set the prefix of the automatically\n",
        "# generated group.id’s via the optional source option groupIdPrefix, default value is “spark-kafka-source”.\n",
        "kafkaConf = {\n",
        "    \"kafka.bootstrap.servers\": kafka_brokers,\n",
        "    # Below settins required if kafka is secured:\n",
        "    # \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    # \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://eventhubname.servicebus.windows.net/;SharedAccessKeyName=listenpolicyforspark;SharedAccessKey=ckNkSjcyXKGN8FCIRIS3qtkKvW+AEhB6QPaM=;EntityPath=instructortest\";',\n",
        "    \"subscribe\": topic_name, # to read from specific partitions use option: \"assign\": {topic_name:[0,1]})\n",
        "    \"startingOffsets\": \"latest\", # \"earliest\", \"latest\"\n",
        "    \"enable.auto.commit\": \"true \",\n",
        "    \"groupIdPrefix\": \"Stream_Analytics_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}\n",
        "\n",
        "\n",
        "# Read from Event Hub using Kafka\n",
        "df = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .options(**kafkaConf)"
      ],
      "metadata": {
        "id": "lnZoZt12M5QI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.load()  # Start reading data from the specified Kafka topic"
      ],
      "metadata": {
        "id": "svKMZkdfM5W9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deserialize the AVRO messages from the value column\n",
        "df = df.select(from_avro(df.value, schema).alias(\"spotify\"))\n",
        "\n",
        "# Print the schema of the DataFrame\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gx_VDGoM5dh",
        "outputId": "24b5550b-dc5d-4832-9d57-8fc568a1bd5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- spotify: struct (nullable = true)\n",
            " |    |-- id: long (nullable = false)\n",
            " |    |-- timestamp: string (nullable = false)\n",
            " |    |-- track_id: string (nullable = false)\n",
            " |    |-- user_id: long (nullable = false)\n",
            " |    |-- listening_time: long (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import  to_timestamp\n",
        "\n",
        "# Update DataFrame to flatten the AVRO record\n",
        "df = df.select(\n",
        "    col(\"spotify.id\").alias(\"event_id\"),\n",
        "    to_timestamp(col(\"spotify.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\").alias(\"event_timestamp\"),\n",
        "    col(\"spotify.track_id\"),\n",
        "    col(\"spotify.user_id\"),\n",
        "    col(\"spotify.listening_time\")\n",
        ")\n",
        "\n",
        "# Print the schema of the updated DataFrame\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxWnJUl7M5kX",
        "outputId": "4cb75bd6-f1ad-4477-d1c1-78940cd1d597"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- event_id: long (nullable = true)\n",
            " |-- event_timestamp: timestamp (nullable = true)\n",
            " |-- track_id: string (nullable = true)\n",
            " |-- user_id: long (nullable = true)\n",
            " |-- listening_time: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoint"
      ],
      "metadata": {
        "id": "GZVMG9shMwL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77403b42-770d-4776-d837-694b65f7f3c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘checkpoint’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the other avro files as spark df:\n",
        "df_tracks = spark.read.format(\"avro\").load(\"/content/drive/MyDrive/stream/data/tracks.avro\")\n",
        "df_users = spark.read.format(\"avro\").load(\"/content/drive/MyDrive/stream/data/users.avro\")\n",
        "df_artists = spark.read.format(\"avro\").load(\"/content/drive/MyDrive/stream/data/artists.avro\")"
      ],
      "metadata": {
        "id": "02SR0DEQMwTs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. PERSONALITY"
      ],
      "metadata": {
        "id": "pnoGFvlKeqKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 F-E SCORE"
      ],
      "metadata": {
        "id": "E3OiwjDbeons"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 GET TOTAL LISTENS OF USER"
      ],
      "metadata": {
        "id": "fehb2S4HPC2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import session_window\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "iSG28-pXjDZF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gap_duration = \"20 minutes\"\n",
        "\n",
        "session_window_df_total_listens = df \\\n",
        "    .withWatermark(\"event_timestamp\", \"5 minutes\") \\\n",
        "    .groupBy('user_id', session_window(\"event_timestamp\", gap_duration).alias(\"session_window\"))\\\n",
        "    .agg(\n",
        "        F.count(\"*\").alias(\"track_count\")\n",
        "    )"
      ],
      "metadata": {
        "id": "rI76FBf7PE-R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUERY TO GET TOTAL LISTENS\n",
        "query_name='user_total_listens'\n",
        "query=session_window_df_total_listens.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .trigger(processingTime='2 seconds')\\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "gzUR8jsDNWX1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_user_total_listens = spark.sql(f\"SELECT * FROM {query_name}\")"
      ],
      "metadata": {
        "id": "zdKqj5A9NXJt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_user_total_listens.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0bxX2J_GdDq",
        "outputId": "be2682f8-007d-4b09-fd1a-1507ba892f53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+-----------+\n",
            "|user_id|session_window|track_count|\n",
            "+-------+--------------+-----------+\n",
            "+-------+--------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_total_listens(user_id):\n",
        "  df_filtered = df_user_total_listens.filter(col(\"user_id\") == user_id)\n",
        "  total_listens = df_filtered.groupBy(\"user_id\").agg(sum(\"track_count\").alias(\"total_track_count\")).collect()[0]\n",
        "\n",
        "  return total_listens[\"total_track_count\"]"
      ],
      "metadata": {
        "id": "pKmzB7sfGTZ8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 GET TOTAL OF UNIQUE SONGS"
      ],
      "metadata": {
        "id": "fSUe6ka_fK7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_listens_unique_artist(user_id):\n",
        "  df_filtered = df_user_total_listens.filter(col(\"user_id\") == user_id)\n",
        "  df_filtered = df_filtered.filter(df_filtered[\"track_count\"] == 1)\n",
        "\n",
        "  return df_filtered.count()"
      ],
      "metadata": {
        "id": "qkXcn4lAI82Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 T-N score"
      ],
      "metadata": {
        "id": "xk-m9i4Yhvgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 GET THE AVERAGE RELEASE DATE OF SONGS IN THE DATASET"
      ],
      "metadata": {
        "id": "sskBsC-LiP1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "llSaciN5jH8u"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUERY TO GET THE AVERAGE RELEASE DATE OF SONGS IN THE DATASET\n",
        "joined_df = df.join(df_tracks, df.track_id == df_tracks.track_id, \"inner\")\n",
        "joined_df = joined_df.withColumn(\"release_year\", col(\"release_date\").cast(\"int\"))\n",
        "average_release_year = joined_df.selectExpr(\"avg(release_year) as avg_release_year\")\n",
        "\n",
        "query_name = \"average_release_year\"\n",
        "query = average_release_year.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "PzZlYVtXiFSl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = \"average_release_year\"\n",
        "df_average_release_year = spark.sql(f\"SELECT * FROM {query_name}\")"
      ],
      "metadata": {
        "id": "nbepUOGPi-oa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_average_release_year.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BBv-NnAKYYW",
        "outputId": "54368dc6-91a9-4fd0-a337-f978249da014"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|avg_release_year|\n",
            "+----------------+\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_average_release_date():\n",
        "  return df_average_release_year.collect()[0][\"avg_release_year\"]"
      ],
      "metadata": {
        "id": "g5vkHbodKflW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 GET THE AVERAGE RELEASE DATE OF SONGS FOR A GIVEN USER"
      ],
      "metadata": {
        "id": "qHsVx-hsjPrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, avg"
      ],
      "metadata": {
        "id": "n2oWLjcUL3L3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = df.join(df_tracks, df.track_id == df_tracks.track_id, \"inner\")\n",
        "joined_df = joined_df.withColumn(\"release_year\", col(\"release_date\").cast(\"int\"))\n",
        "average_release_date_df = joined_df.groupBy(\"user_id\").agg(avg(\"release_date\").alias(\"average_release_date\"))"
      ],
      "metadata": {
        "id": "grfMMbusMCiQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = \"average_release_date_df\"\n",
        "query = average_release_date_df.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "XPDBcpgwMJgx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = \"average_release_date_df\"\n",
        "df_average_release_date_by_user = spark.sql(f\"SELECT * FROM {query_name}\")"
      ],
      "metadata": {
        "id": "Gz2DdO5vMdlj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_average_release_date_by_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqL5bgmCMntH",
        "outputId": "6e18f854-175e-46ff-f96e-bd4a0974f493"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "|user_id|average_release_date|\n",
            "+-------+--------------------+\n",
            "+-------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_average_release_date(user_id):\n",
        "  df_filtered = df_average_release_date_by_user.filter(col(\"user_id\") == user_id).collect()[0]\n",
        "\n",
        "  return df_filtered[\"average_release_date\"]"
      ],
      "metadata": {
        "id": "emB-jPmkK649"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 L-V score"
      ],
      "metadata": {
        "id": "QLONiHpzlBL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOYALTY_COUNTER\n",
        "For every user, for every listen. If listen is of the same artist than the previous listen, add 1 to the counter."
      ],
      "metadata": {
        "id": "8yitKpcDlG2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_loyalty_counter(user_id):\n",
        "    df_user = df.filter(df.user_id == str(user_id))\n",
        "    joined_df = df_user.join(df_tracks, df_user.track_id == df_tracks.track_id, \"inner\")\n",
        "\n",
        "    query_name=f'user{str(user_id)}_loyalty_counter'\n",
        "    try:\n",
        "      streaming_df = joined_df.writeStream \\\n",
        "        .outputMode(\"append\") \\\n",
        "        .format(\"memory\") \\\n",
        "        .queryName(query_name) \\\n",
        "        .start()\n",
        "    except Exception as e:\n",
        "      print(f\"Can't create query because exception: {e}\")\n",
        "\n",
        "    query = f\"\"\"\n",
        "        SELECT count(*) AS count\n",
        "        FROM (\n",
        "            SELECT event_timestamp, artist,\n",
        "                  lag(artist) OVER (ORDER BY event_timestamp) AS previous_artist\n",
        "            FROM {query_name}\n",
        "        ) WHERE artist = previous_artist\n",
        "    \"\"\"\n",
        "\n",
        "    output = spark.sql(query)\n",
        "    loyalty_counter = int(output.first()[\"count\"])\n",
        "\n",
        "    return loyalty_counter"
      ],
      "metadata": {
        "id": "adg6KOZHWBZa"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 C-U score"
      ],
      "metadata": {
        "id": "i3jbYB_KlPLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GET NUMBER OF LISTENS TO MAINSTREAM SONGS BY USER"
      ],
      "metadata": {
        "id": "GjIgBtNt4EYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = df.join(df_tracks, df.track_id == df_tracks.track_id, \"inner\")"
      ],
      "metadata": {
        "id": "p2PVoU5t5Xos"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streaming_df = joined_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"artists\") \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "bhMUbSyH5Z7Z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_name = \"artists\"\n",
        "df_mainstream_artists = spark.sql(f\"SELECT * FROM {query_name}\")"
      ],
      "metadata": {
        "id": "Fku6MewKQNkH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mainstream_artists.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H30a1cUSQYSG",
        "outputId": "e2954e47-1f0c-4ff1-e961-6e3511daf3bc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------------+--------+-------+--------------+--------+--------+------+----+----------+------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-----+--------------+----------+-----------+\n",
            "|event_id|event_timestamp|track_id|user_id|listening_time|track_id|duration|artist|name|popularity|release_date|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|tempo|time_signature|album_name|track_genre|\n",
            "+--------+---------------+--------+-------+--------------+--------+--------+------+----+----------+------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-----+--------------+----------+-----------+\n",
            "+--------+---------------+--------+-------+--------------+--------+--------+------+----+----------+------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-----+--------------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_mainstream_counter(user_id):\n",
        "  df_filtered = df_mainstream_artists.filter(col(\"user_id\") == user_id)\n",
        "\n",
        "  return df_filtered.filter(col(\"popularity\") > 50).count()"
      ],
      "metadata": {
        "id": "nIJXcGBCQwCI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. ANALYTICS"
      ],
      "metadata": {
        "id": "2ciiUvKdH32_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP"
      ],
      "metadata": {
        "id": "Ii-wx4dcIoMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import window, col, min, max, count, unix_timestamp, collect_list, map_from_arrays\n",
        "spark.conf.set(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\")\n",
        "\n",
        "\n",
        "query_name = 'df'\n",
        "query = df.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_name) \\\n",
        "    .start()\n",
        "\n",
        "df_final = spark.sql(\"SELECT * FROM df\")\n",
        "# Apply transformations to create session_window_df\n",
        "session_window_df = df_final \\\n",
        "    .withWatermark(\"event_timestamp\", \"5 minutes\") \\\n",
        "    .groupBy('user_id', session_window(\"event_timestamp\", \"20 minutes\").alias(\"session_window\"), \"track_id\") \\\n",
        "    .agg(\n",
        "        min(\"event_timestamp\").alias(\"window_start\"),\n",
        "        max(\"event_timestamp\").alias(\"window_end\"),\n",
        "        (unix_timestamp(max(\"event_timestamp\")) - unix_timestamp(min(\"event_timestamp\"))).alias(\"session_duration\"),\n",
        "        count(\"*\").alias(\"track_count\")\n",
        "    )\n",
        "\n",
        "# Aggregate track counts into a map-type column\n",
        "session_window_dict_df = session_window_df \\\n",
        "    .groupBy('user_id', 'session_window') \\\n",
        "    .agg(\n",
        "        map_from_arrays(collect_list('track_id'), collect_list('track_count')).alias('track_count_dict'),\n",
        "        min('window_start').alias('window_start'),\n",
        "        max('window_end').alias('window_end'),\n",
        "        max('session_duration').alias('session_duration')\n",
        "    )\n",
        "\n",
        "# Write the DataFrame to a memory table\n",
        "session_window_dict_df.createOrReplaceTempView(\"spotify_query_with_tracks_dict\")\n",
        "\n",
        "# Query the memory table\n",
        "result_df = spark.sql(\"SELECT * FROM spotify_query_with_tracks_dict\")\n"
      ],
      "metadata": {
        "id": "tLowpaJEH_xo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI7jwcSgIKqW",
        "outputId": "16c867d3-5791-4603-919c-6b1ae11504e7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_name_parquet = \"df_parquet\"\n",
        "query_parquet = df \\\n",
        "    .writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"checkpointLocation\", \"checkpoint3\") \\\n",
        "    .option(\"path\", \"/content/events_df\") \\\n",
        "    .queryName(query_name_parquet) \\\n",
        "    .start()"
      ],
      "metadata": {
        "id": "iIREMglpINsc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_df = spark.read.parquet('/content/events_df')\n",
        "parquet_df.show(50, truncate=False)"
      ],
      "metadata": {
        "id": "CIEpvAY0IO0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 GENERAL ANALYTICS"
      ],
      "metadata": {
        "id": "B9K006o006jH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0.1  Top 5 most streamed songs per week"
      ],
      "metadata": {
        "id": "VyiEFut40-NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window"
      ],
      "metadata": {
        "id": "IgKhP2DedBE3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, weekofyear, collect_list, rank, desc"
      ],
      "metadata": {
        "id": "dVnufHxm1I2m"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df = result_df.select(\"user_id\", \"session_window\", explode(\"track_count_dict\").alias(\"track_id\", \"count\"))\n",
        "\n",
        "# Extract week from session_window\n",
        "exploded_df = exploded_df.withColumn(\"week\", weekofyear(\"session_window.start\"))\n",
        "\n",
        "# Group by week and track_id to calculate the total count of each track for each week\n",
        "week_grouped_df = exploded_df.groupby(\"week\", \"track_id\").agg({\"count\": \"sum\"})\n",
        "\n",
        "# Rank the songs for each week\n",
        "window_spec = Window.partitionBy(\"week\").orderBy(desc(\"sum(count)\"))\n",
        "ranked_songs_df = week_grouped_df.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "# Filter to select the top 5 songs for each week\n",
        "top_5_songs_df = ranked_songs_df.filter(col(\"rank\") <= 5)\n",
        "\n",
        "# Collect the top 5 songs for each week into a list\n",
        "top_5_songs_list_per_week = top_5_songs_df.groupby(\"week\").agg(collect_list(\"track_id\").alias(\"top_5_songs\"))\n",
        "\n",
        "top_5_songs_with_names_df = top_5_songs_df.join(df_tracks, \"track_id\", \"left\")\n",
        "\n",
        "# Select the necessary columns\n",
        "top_5_songs_names_df = top_5_songs_with_names_df.select(\"week\", \"name\")\n",
        "\n",
        "# Group by week and collect the names into a list\n",
        "top_5_songs_list_per_week_with_names = top_5_songs_names_df.groupby(\"week\") \\\n",
        "    .agg(collect_list(\"name\").alias(\"top_5_song_names\"))\n",
        "\n",
        "# Show the result\n",
        "top_5_songs_list_per_week_with_names.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3UC6DXk1Dkq",
        "outputId": "a65fa6f1-b5f7-4e6d-8876-1abde34a2c53"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------------------+\n",
            "|week|top_5_song_names                 |\n",
            "+----+---------------------------------+\n",
            "|6   |[How Can You Mend A Broken Heart]|\n",
            "+----+---------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_5_streamed_songs_current_week():\n",
        "  max_week = top_5_songs_list_per_week_with_names.select(max(\"week\")).collect()[0][0]\n",
        "  latest_row = top_5_songs_list_per_week_with_names.filter(col(\"week\") == max_week).first()\n",
        "\n",
        "  return latest_row[\"top_5_song_names\"]"
      ],
      "metadata": {
        "id": "UItJW9oO1KLu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0.2 Top 5 most streamed artists per week"
      ],
      "metadata": {
        "id": "hXqqU9MK2KKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, weekofyear, collect_list, rank, desc"
      ],
      "metadata": {
        "id": "sGxMJlAh3umN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df = result_df.select(\"user_id\", \"session_window\", explode(\"track_count_dict\").alias(\"track_id\", \"count\"))\n",
        "\n",
        "# Extract week from session_window\n",
        "exploded_df = exploded_df.withColumn(\"week\", weekofyear(\"session_window.start\"))\n",
        "\n",
        "# Join with df_tracks to get artist information\n",
        "joined_df = exploded_df.join(df_tracks, \"track_id\", \"left\")\n",
        "\n",
        "# Group by week and artist to calculate the total count of each artist for each week\n",
        "week_artist_grouped_df = joined_df.groupby(\"week\", \"artist\").agg({\"count\": \"sum\"})\n",
        "\n",
        "# Rank the artists for each week\n",
        "window_spec = Window.partitionBy(\"week\").orderBy(desc(\"sum(count)\"))\n",
        "ranked_artists_df = week_artist_grouped_df.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "# Filter to select the top 5 artists for each week\n",
        "top_5_artists_df = ranked_artists_df.filter(col(\"rank\") <= 5)\n",
        "\n",
        "# Collect the top 5 artists for each week into a list\n",
        "top_5_artists_list_per_week = top_5_artists_df.groupby(\"week\").agg(collect_list(\"artist\").alias(\"top_5_artists\"))\n",
        "\n",
        "# Show the result\n",
        "top_5_artists_list_per_week.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwRu5xbk2JpB",
        "outputId": "29aa8151-5752-4703-ced0-17c3ee6c66f2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------------------------------------------------------------------------------------------------------------+\n",
            "|week|top_5_artists                                                                                                           |\n",
            "+----+------------------------------------------------------------------------------------------------------------------------+\n",
            "|6   |[['McCoy Tyner'], ['Belle and the Nursery Rhymes Band'], ['Unnikrishnan', 'Sujatha'], ['Fred Åkerström'], ['Jazzanova']]|\n",
            "+----+------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_5_streamed_artists_current_week():\n",
        "  max_week = top_5_artists_list_per_week.select(max(\"week\")).collect()[0][0]\n",
        "  latest_row = top_5_artists_list_per_week.filter(col(\"week\") == max_week).first()\n",
        "\n",
        "  return latest_row[\"top_5_artists\"]"
      ],
      "metadata": {
        "id": "YYIgwLQU3vdc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0.3 Top 5 songs streamed per month"
      ],
      "metadata": {
        "id": "ycBQtPig4hB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month, dense_rank"
      ],
      "metadata": {
        "id": "f9lRXKTI4mnN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract month from session_window\n",
        "exploded_df = exploded_df.withColumn(\"month\", month(\"session_window.start\"))\n",
        "\n",
        "# Group by month and track_id to calculate the total count of each track for each month\n",
        "month_grouped_df = exploded_df.groupby(\"month\", \"track_id\").agg({\"count\": \"sum\"})\n",
        "\n",
        "# Rank the songs for each month\n",
        "window_spec_month = Window.partitionBy(\"month\").orderBy(desc(\"sum(count)\"))\n",
        "ranked_songs_month_df = month_grouped_df.withColumn(\"rank\", dense_rank().over(window_spec_month))\n",
        "\n",
        "# Filter to select the top 5 songs for each month\n",
        "top_5_songs_month_df = ranked_songs_month_df.filter(col(\"rank\") <= 5)\n",
        "\n",
        "# Join with df_tracks to get the names of the top 5 songs for each month\n",
        "top_5_songs_with_names_month_df = top_5_songs_month_df.join(df_tracks, \"track_id\", \"left\")\n",
        "\n",
        "# Select the necessary columns\n",
        "top_5_songs_names_month_df = top_5_songs_with_names_month_df.select(\"month\", \"name\")\n",
        "\n",
        "# Group by month and collect the names into a list\n",
        "top_5_songs_list_per_month_with_names = top_5_songs_names_month_df.groupby(\"month\") \\\n",
        "    .agg(collect_list(\"name\").alias(\"top_5_song_names\"))\n",
        "\n",
        "# Show the result\n",
        "top_5_songs_list_per_month_with_names.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCLru2jZ4kxM",
        "outputId": "ff008653-9ff8-42a4-db97-c50bcd16f8e7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------------------------------------------------------------------------------------------------+\n",
            "|month|top_5_song_names                                                                                   |\n",
            "+-----+---------------------------------------------------------------------------------------------------+\n",
            "|2    |[Contemplation, Reykjavik, Esmer, Us Against The World (Da Tweekaz Remix), Loving Strangers, Kukla]|\n",
            "+-----+---------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_5_streamed_songs_current_month():\n",
        "  max_month = top_5_songs_list_per_month_with_names.select(max(\"month\")).collect()[0][0]\n",
        "  latest_row = top_5_songs_list_per_month_with_names.filter(col(\"month\") == max_month).first()\n",
        "\n",
        "  return latest_row[\"top_5_song_names\"]"
      ],
      "metadata": {
        "id": "loK3Q5MR4pjn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.0.4 Top 5 streamed artist per month"
      ],
      "metadata": {
        "id": "ocNB04OD5Jni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, month, collect_list, rank, desc"
      ],
      "metadata": {
        "id": "m-jUUM1z5Syj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df = result_df.select(\"user_id\", \"session_window\", explode(\"track_count_dict\").alias(\"track_id\", \"count\"))\n",
        "\n",
        "# Extract month from session_window\n",
        "exploded_df = exploded_df.withColumn(\"month\", month(\"session_window.start\"))\n",
        "\n",
        "# Join with df_tracks to get artist information\n",
        "joined_df = exploded_df.join(df_tracks, \"track_id\", \"left\")\n",
        "\n",
        "# Group by month and artist to calculate the total count of each artist for each month\n",
        "month_artist_grouped_df = joined_df.groupby(\"month\", \"artist\").agg({\"count\": \"sum\"})\n",
        "\n",
        "# Rank the artists for each month\n",
        "window_spec = Window.partitionBy(\"month\").orderBy(desc(\"sum(count)\"))\n",
        "ranked_artists_df = month_artist_grouped_df.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "# Filter to select the top 5 artists for each month\n",
        "top_5_artists_df = ranked_artists_df.filter(col(\"rank\") <= 5)\n",
        "\n",
        "# Collect the top 5 artists for each month into a list\n",
        "top_5_artists_list_per_month = top_5_artists_df.groupby(\"month\").agg(collect_list(\"artist\").alias(\"top_5_artists\"))\n",
        "\n",
        "# Show the result\n",
        "top_5_artists_list_per_month.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8r8S2-w5O4M",
        "outputId": "c8cbcee3-1d2c-472c-bf13-dac85037fdd4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------------------------------------------------------------------------------------------+\n",
            "|month|top_5_artists                                                                                                      |\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------+\n",
            "|2    |[['Ceylan Ertem'], ['McCoy Tyner'], ['Old Harp Singers of Eastern Tennessee'], ['Gunnar Þórðarson'], ['Clockartz']]|\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_5_streamed_artists_current_month():\n",
        "  max_month = top_5_artists_list_per_month.select(max(\"month\")).collect()[0][0]\n",
        "  latest_row = top_5_artists_list_per_month.filter(col(\"month\") == max_month).first()\n",
        "\n",
        "  return latest_row[\"top_5_artists\"]"
      ],
      "metadata": {
        "id": "9uaskmEF5T_v"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Weekly User Queries"
      ],
      "metadata": {
        "id": "KTPUSk4sImG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Top 5 songs per user per week"
      ],
      "metadata": {
        "id": "mJ4VYAkuI5sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, row_number, col, weekofyear, max\n",
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "UWsP60nHsQFF"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_latest_week(df, user_id):\n",
        "  filtered_df = df.filter(col(\"user_id\") == user_id)\n",
        "  max_week = filtered_df.select(max(\"week\")).collect()[0][0]\n",
        "  latest_row = filtered_df.filter(col(\"week\") == max_week).first()\n",
        "  return latest_row"
      ],
      "metadata": {
        "id": "zDf8wrgAw6ZD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df = result_df.withColumn(\"track_id\", explode(F.map_keys(\"track_count_dict\"))) \\\n",
        "                      .withColumn(\"count\", explode(F.map_values(\"track_count_dict\")))\n",
        "\n",
        "# Define a window specification to partition by user_id and week and order by the count descending\n",
        "window_spec = Window.partitionBy(\"user_id\", weekofyear(\"session_window.start\")).orderBy(F.desc(\"count\"))\n",
        "\n",
        "# Rank the top 5 songs for each user and each week\n",
        "ranked_songs_df = exploded_df.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
        "                             .filter(F.col(\"rank\") <= 5)\n",
        "\n",
        "# Join with the DataFrame containing song information to get the song name\n",
        "top_songs_with_names_df = ranked_songs_df.join(df_tracks, \"track_id\", \"left\")\n",
        "\n",
        "# Aggregate the counts of the top 5 songs for each user and each week\n",
        "top_songs_count_df = top_songs_with_names_df.groupBy(\"user_id\", weekofyear(\"session_window.start\").alias(\"week\")) \\\n",
        "                                             .agg(F.collect_list(\"name\").alias(\"top_songs\"),\n",
        "                                                  F.collect_list(\"count\").alias(\"count\"))\n",
        "\n",
        "# Show the result\n",
        "top_songs_count_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSp4vHN7IxME",
        "outputId": "afecea10-a161-40e3-d9a2-11513b0bba01"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "|user_id|week|top_songs                                                                                                       |count               |\n",
            "+-------+----+----------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "|0      |6   |[Amazing Grace, Amazing Grace, Dónde Estás, Once I Caught a Fish Alive, Acid Thunder]                           |[8, 8, 8, 7, 7]     |\n",
            "|1      |6   |[Deep Waters, Smiling Faces, Bullet, I'm Not the Man I'm Supposed to Be, Serenata]                              |[5, 5, 5, 5, 5]     |\n",
            "|2      |6   |[Delirium, Us Against The World (Da Tweekaz Remix), Delirium, Us Against The World (Da Tweekaz Remix), Delirium]|[13, 8, 7, 5, 5]    |\n",
            "|3      |6   |[Loving Strangers, Kamini, Rockin' Around the Christmas Tree, Heading Home, Loving Strangers]                   |[10, 10, 10, 10, 6] |\n",
            "|4      |6   |[Chantilly Lace, White Lightnin', White Lightnin', Chantilly Lace, stay with me]                                |[14, 14, 10, 10, 6] |\n",
            "|5      |6   |[Macarena, Necesitamos Mas Acción, La Candela, Não Vá Se Perder Por Aí, Avarandado]                             |[15, 15, 15, 15, 15]|\n",
            "|6      |6   |[Poovukkul, Ave Maria, Alberto Balsalm, Jogi, Showdown]                                                         |[13, 13, 10, 10, 10]|\n",
            "|7      |6   |[Contemplation, Aria, Sad, My Own Way, Esmer]                                                                   |[18, 10, 10, 10, 9] |\n",
            "|8      |6   |[Tie, Tie, Bohemian Sunset, Drew Barrymore, Severek Ayrılanlar]                                                 |[19, 10, 8, 8, 8]   |\n",
            "|9      |6   |[Reykjavik, How Can You Mend A Broken Heart, Reykjavik, Oslo, Berceuse]                                         |[14, 14, 11, 7, 7]  |\n",
            "+-------+----+----------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_top_5_songs_current_week(user_id):\n",
        "  latest_row = get_user_latest_week(top_songs_count_df, user_id)\n",
        "\n",
        "  return latest_row[\"top_songs\"]"
      ],
      "metadata": {
        "id": "6EXxkJlyI_DA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Weekly listening time / count of songs"
      ],
      "metadata": {
        "id": "eOJ7H0c2uMsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, weekofyear, sum"
      ],
      "metadata": {
        "id": "P3T7IXEju3qo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df_week = result_df.withColumn(\"track_id\", explode(F.map_keys(\"track_count_dict\"))) \\\n",
        "                            .withColumn(\"count\", explode(F.map_values(\"track_count_dict\")))\n",
        "\n",
        "# Define a window specification to partition by user_id and week\n",
        "window_spec_week = Window.partitionBy(\"user_id\", weekofyear(\"session_window.start\"))\n",
        "\n",
        "# Aggregate the listening time and total count of all songs for each user and week\n",
        "aggregated_df_week = exploded_df_week.groupBy(\"user_id\", weekofyear(\"session_window.start\").alias(\"week\")) \\\n",
        "                                     .agg(sum(\"session_duration\").alias(\"listening_time\"),\n",
        "                                          sum(\"count\").alias(\"total_song_count\"))\n",
        "\n",
        "# Show the result\n",
        "aggregated_df_week.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQvdeK7wuMIo",
        "outputId": "5f823cc0-dff8-4e83-9fdb-953050edcc2e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+--------------+----------------+\n",
            "|user_id|week|listening_time|total_song_count|\n",
            "+-------+----+--------------+----------------+\n",
            "|      0|   6|        105127|             314|\n",
            "|      1|   6|        219848|             435|\n",
            "|      2|   6|         33977|             197|\n",
            "|      3|   6|         33823|             104|\n",
            "|      4|   6|         82678|             264|\n",
            "|      5|   6|        415880|             423|\n",
            "|      6|   6|         81734|             215|\n",
            "|      7|   6|        138808|             327|\n",
            "|      8|   6|        346019|             562|\n",
            "|      9|   6|         96448|             275|\n",
            "+-------+----+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_listening_time_current_week(user_id):\n",
        "  latest_row = get_user_latest_week(aggregated_df_week, user_id)\n",
        "\n",
        "  return latest_row[\"listening_time\"]"
      ],
      "metadata": {
        "id": "-f6R-BLuvbXe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_total_listens_current_week(user_id):\n",
        "  latest_row = get_user_latest_week(aggregated_df_week, user_id)\n",
        "\n",
        "  return latest_row[\"total_song_count\"]"
      ],
      "metadata": {
        "id": "enXB4RVAvgMB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Percentile Analysis"
      ],
      "metadata": {
        "id": "iRihI3XIyTdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col, weekofyear, when ,expr\n",
        "from pyspark.sql.window import Window\n",
        "from tdigest import TDigest"
      ],
      "metadata": {
        "id": "dsjX0QsbzHnc"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to compute the percentile using T-Digest\n",
        "def compute_percentile(counts):\n",
        "    tdigest = TDigest()\n",
        "    for count in counts:\n",
        "        tdigest.update(count)\n",
        "    return tdigest.percentile(0.99)\n",
        "\n",
        "# UDF to apply the percentile function\n",
        "compute_percentile_udf = F.udf(compute_percentile)\n",
        "\n",
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df = result_df.withColumn(\"track_id\", explode(F.map_keys(\"track_count_dict\"))) \\\n",
        "                        .withColumn(\"count\", explode(F.map_values(\"track_count_dict\")))\n",
        "\n",
        "# Define a window specification to partition by user_id and week and order by the count descending\n",
        "window_spec = Window.partitionBy(\"user_id\", weekofyear(\"session_window.start\")).orderBy(col(\"count\").desc())\n",
        "\n",
        "# Rank the songs for each user and each week\n",
        "ranked_songs_df = exploded_df.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "\n",
        "# Select the top 1 song for each user and each week\n",
        "top_songs_df = ranked_songs_df.filter(col(\"rank\") == 1)\n",
        "\n",
        "# Aggregate the counts of the top 1 song for each user, week, and track\n",
        "top_songs_count_df = top_songs_df.groupBy(\"user_id\", weekofyear(\"session_window.start\").alias(\"week\"), \"track_id\") \\\n",
        "                                  .agg(F.sum(\"count\").alias(\"count\"))\n",
        "\n",
        "top_songs_with_names_df = top_songs_count_df.join(df_tracks, \"track_id\", \"left\") \\\n",
        "                                             .withColumnRenamed(\"name\", \"song_name\") \\\n",
        "                                             .select(\"user_id\", \"week\", \"song_name\", \"count\")\n",
        "\n",
        "# Compute the 95th percentile for each user, week, and track\n",
        "percentile_df = top_songs_with_names_df.groupBy(\"user_id\", \"week\", \"song_name\").agg(F.collect_list(\"count\").alias(\"counts\")) \\\n",
        "                                  .withColumn(\"percentile\", compute_percentile_udf(\"counts\").cast(\"double\"))\n",
        "\n",
        "\n",
        "# Add a new column indicating whether the count is equal to the percentile\n",
        "results_df = percentile_df.withColumn(\"top_percentile\",\n",
        "                                     expr(\"transform(counts, x -> if(x = percentile, 'Top 1%', 'Not Top 1%'))\"))\n",
        "\n",
        "# Show the result with all required columns\n",
        "results_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQIGcCvxyQbx",
        "outputId": "3af5abb6-e5cc-4f7c-9afe-975f8804dad8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+--------------------+------+----------+--------------+\n",
            "|user_id|week|song_name           |counts|percentile|top_percentile|\n",
            "+-------+----+--------------------+------+----------+--------------+\n",
            "|0      |6   |Dónde Estás         |[9]   |9.0       |[Top 1%]      |\n",
            "|1      |6   |Twilight            |[8]   |8.0       |[Top 1%]      |\n",
            "|2      |6   |Delirium            |[13]  |13.0      |[Top 1%]      |\n",
            "|3      |6   |Escrito Nas Estrelas|[13]  |13.0      |[Top 1%]      |\n",
            "|4      |6   |Chantilly Lace      |[14]  |14.0      |[Top 1%]      |\n",
            "|5      |6   |Macarena            |[15]  |15.0      |[Top 1%]      |\n",
            "|6      |6   |Poovukkul           |[13]  |13.0      |[Top 1%]      |\n",
            "|7      |6   |Contemplation       |[18]  |18.0      |[Top 1%]      |\n",
            "|8      |6   |Tie                 |[19]  |19.0      |[Top 1%]      |\n",
            "|9      |6   |Reykjavik           |[14]  |14.0      |[Top 1%]      |\n",
            "+-------+----+--------------------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_top_song_percentile_current_week(user_id):\n",
        "  latest_row = get_user_latest_week(results_df, user_id)\n",
        "\n",
        "  return latest_row[\"song_name\"], latest_row[\"top_percentile\"][0]\n"
      ],
      "metadata": {
        "id": "A8CdiRrEzQfp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.4 Engagement Metrics user_id vs all users"
      ],
      "metadata": {
        "id": "l_RuSAE45163"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import weekofyear, col\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "caK2sjWk59K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the user_id\n",
        "user_id = \"1\""
      ],
      "metadata": {
        "id": "BsTFwG526drx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.withColumn(\"session_duration_minutes\", col(\"session_duration\") / 60)"
      ],
      "metadata": {
        "id": "Q-2IRAuH7in-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ztDzBit9e_E",
        "outputId": "026125bf-11a4-4606-c907-cf6b6888093f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[user_id: bigint, session_window: struct<start:timestamp,end:timestamp>, track_count_dict: map<string,bigint>, window_start: timestamp, window_end: timestamp, session_duration: bigint, session_duration_minutes: double]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter result_df for the specific user_id\n",
        "user_result_df = result_df.filter(col(\"user_id\") == user_id)\n",
        "\n",
        "# Calculate session duration for the specific user\n",
        "user_result_df = user_result_df.withColumn(\"session_duration_minutes\", (col(\"session_window.end\").cast(\"long\") - col(\"session_window.start\").cast(\"long\")) / 60)\n",
        "\n",
        "# Extract week from session window for the specific user\n",
        "user_result_df = user_result_df.withColumn(\"week\", weekofyear(\"session_window.start\"))\n",
        "\n",
        "# Group by week and calculate average session duration for the specific user\n",
        "average_session_duration_per_week_user = user_result_df.groupby(\"week\").agg({\"session_duration_minutes\": \"avg\"})\n",
        "\n",
        "# Order by week for the specific user\n",
        "average_session_duration_per_week_user = average_session_duration_per_week_user.orderBy(\"week\")\n",
        "\n",
        "# Convert to Pandas DataFrame for plotting for the specific user\n",
        "average_session_duration_per_week_pd_user = average_session_duration_per_week_user.toPandas()\n",
        "\n",
        "# Group by week and calculate average session duration for all users\n",
        "average_session_duration_per_week_all = result_df.groupby(\"week\").agg({\"session_duration_minutes\": \"avg\"})\n",
        "\n",
        "# Order by week for all users\n",
        "average_session_duration_per_week_all = average_session_duration_per_week_all.orderBy(\"week\")\n",
        "\n",
        "# Convert to Pandas DataFrame for plotting for all users\n",
        "average_session_duration_per_week_pd_all = average_session_duration_per_week_all.toPandas()\n",
        "\n",
        "# Plot both lines in one plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot average session duration per week for all users\n",
        "plt.plot(average_session_duration_per_week_pd_all[\"week\"], average_session_duration_per_week_pd_all[\"avg(session_duration_minutes)\"], marker='o', label=\"All Users\")\n",
        "\n",
        "# Plot average session duration per week for the specific user\n",
        "plt.plot(average_session_duration_per_week_pd_user[\"week\"], average_session_duration_per_week_pd_user[\"avg(session_duration_minutes)\"], marker='s', label=f\"User {user_id}\")\n",
        "\n",
        "plt.xlabel(\"Week\")\n",
        "plt.ylabel(\"Average Session Duration (minutes)\")\n",
        "plt.title(\"Average Session Duration Per Week\")\n",
        "plt.xticks(average_session_duration_per_week_pd_all[\"week\"])\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "rAu8QplK56Pt",
        "outputId": "ee7c6dbb-9f77-4a6d-ee31-e98ee209d34d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `week` cannot be resolved. Did you mean one of the following? [`session_duration_minutes`, `spotify_query_with_tracks_dict`.`user_id`, `spotify_query_with_tracks_dict`.`window_end`, `spotify_query_with_tracks_dict`.`window_start`, `spotify_query_with_tracks_dict`.`session_window`].;\n'Aggregate ['week], ['week, avg(session_duration_minutes#188212) AS avg(session_duration_minutes)#188911]\n+- Project [user_id#1507L, session_window#1520, track_count_dict#1569, window_start#1571, window_end#1573, session_duration#1575L, (cast(session_duration#1575L as double) / cast(60 as double)) AS session_duration_minutes#188212]\n   +- Project [user_id#1507L, session_window#1520, track_count_dict#1569, window_start#1571, window_end#1573, session_duration#1575L]\n      +- SubqueryAlias spotify_query_with_tracks_dict\n         +- View (`spotify_query_with_tracks_dict`, [user_id#1507L,session_window#1520,track_count_dict#1569,window_start#1571,window_end#1573,session_duration#1575L])\n            +- Aggregate [user_id#1507L, session_window#1520], [user_id#1507L, session_window#1520, map_from_arrays(collect_list(track_id#1506, 0, 0), collect_list(track_count#1534L, 0, 0)) AS track_count_dict#1569, min(window_start#1527) AS window_start#1571, max(window_end#1529) AS window_end#1573, max(session_duration#1532L) AS session_duration#1575L]\n               +- Aggregate [user_id#1507L, session_window#1535, track_id#1506], [user_id#1507L, session_window#1535 AS session_window#1520, track_id#1506, min(event_timestamp#1505) AS window_start#1527, max(event_timestamp#1505) AS window_end#1529, (unix_timestamp(max(event_timestamp#1505), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) - unix_timestamp(min(event_timestamp#1505), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false)) AS session_duration#1532L, count(1) AS track_count#1534L]\n                  +- Filter isnotnull(event_timestamp#1505)\n                     +- Project [named_struct(start, precisetimestampconversion(precisetimestampconversion(event_timestamp#1505, TimestampType, LongType), LongType, TimestampType), end, knownnullable(precisetimestampconversion(precisetimestampconversion(cast(event_timestamp#1505 + cast(20 minutes as interval) as timestamp), TimestampType, LongType), LongType, TimestampType))) AS session_window#1535, event_id#1504L, event_timestamp#1505, track_id#1506, user_id#1507L, listening_time#1508L]\n                        +- Project [event_id#1504L, event_timestamp#1505, track_id#1506, user_id#1507L, listening_time#1508L]\n                           +- SubqueryAlias df\n                              +- View (`df`, [event_id#1504L,event_timestamp#1505,track_id#1506,user_id#1507L,listening_time#1508L])\n                                 +- MemoryPlan MemorySink, [event_id#1504L, event_timestamp#1505, track_id#1506, user_id#1507L, listening_time#1508L]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-31b24e816d91>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Group by week and calculate average session duration for all users\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0maverage_session_duration_per_week_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"week\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"session_duration_minutes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"avg\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Order by week for all users\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exprs should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;31m# Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `week` cannot be resolved. Did you mean one of the following? [`session_duration_minutes`, `spotify_query_with_tracks_dict`.`user_id`, `spotify_query_with_tracks_dict`.`window_end`, `spotify_query_with_tracks_dict`.`window_start`, `spotify_query_with_tracks_dict`.`session_window`].;\n'Aggregate ['week], ['week, avg(session_duration_minutes#188212) AS avg(session_duration_minutes)#188911]\n+- Project [user_id#1507L, session_window#1520, track_count_dict#1569, window_start#1571, window_end#1573, session_duration#1575L, (cast(session_duration#1575L as double) / cast(60 as double)) AS session_duration_minutes#188212]\n   +- Project [user_id#1507L, session_window#1520, track_count_dict#1569, window_start#1571, window_end#1573, session_duration#1575L]\n      +- SubqueryAlias spotify_query_with_tracks_dict\n         +- View (`spotify_query_with_tracks_dict`, [user_id#1507L,session_window#1520,track_count_dict#1569,window_start#1571,window_end#1573,session_duration#1575L])\n            +- Aggregate [user_id#1507L, session_window#1520], [user_id#1507L, session_window#1520, map_from_arrays(collect_list(track_id#1506, 0, 0), collect_list(track_count#1534L, 0, 0)) AS track_count_dict#1569, min(window_start#1527) AS window_start#1571, max(window_end#1529) AS window_end#1573, max(session_duration#1532L) AS session_duration#1575L]\n               +- Aggregate [user_id#1507L, session_window#1535, track_id#1506], [user_id#1507L, session_window#1535 AS session_window#1520, track_id#1506, min(event_timestamp#1505) AS window_start#1527, max(event_timestamp#1505) AS window_end#1529, (unix_timestamp(max(event_timestamp#1505), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) - unix_timestamp(min(event_timestamp#1505), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false)) AS session_duration#1532L, count(1) AS track_count#1534L]\n                  +- Filter isnotnull(event_timestamp#1505)\n                     +- Project [named_struct(start, precisetimestampconversion(precisetimestampconversion(event_timestamp#1505, TimestampType, LongType), LongType, TimestampType), end, knownnullable(precisetimestampconversion(precisetimestampconversion(cast(event_timestamp#1505 + cast(20 minutes as interval) as timestamp), TimestampType, LongType), LongType, TimestampType))) AS session_window#1535, event_id#1504L, event_timestamp#1505, track_id#1506, user_id#1507L, listening_time#1508L]\n                        +- Project [event_id#1504L, event_timestamp#1505, track_id#1506, user_id#1507L, listening_time#1508L]\n                           +- SubqueryAlias df\n                              +- View (`df`, [event_id#1504L,event_timestamp#1505,track_id#1506,user_id#1507L,listening_time#1508L])\n                                 +- MemoryPlan MemorySink, [event_id#1504L, event_timestamp#1505, track_id#1506, user_id#1507L, listening_time#1508L]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Monthly Queries"
      ],
      "metadata": {
        "id": "18_xADgeIvGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Top 5 songs per user per week"
      ],
      "metadata": {
        "id": "xPjpOHSw_wxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, row_number, month"
      ],
      "metadata": {
        "id": "ip4SYEzg_22_"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_latest_month(df, user_id):\n",
        "  filtered_df = df.filter(col(\"user_id\") == user_id)\n",
        "  max_month = filtered_df.select(max(\"month\")).collect()[0][0]\n",
        "  latest_row = filtered_df.filter(col(\"month\") == max_month).first()\n",
        "  return latest_row"
      ],
      "metadata": {
        "id": "cq6f86qu_7Sq"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df_month = result_df.withColumn(\"track_id\", explode(F.map_keys(\"track_count_dict\"))) \\\n",
        "                      .withColumn(\"count\", explode(F.map_values(\"track_count_dict\")))\n",
        "\n",
        "# Define a window specification to partition by user_id and month and order by the count descending\n",
        "window_spec_month = Window.partitionBy(\"user_id\", month(\"session_window.start\")).orderBy(F.desc(\"count\"))\n",
        "\n",
        "# Rank the top 5 songs for each user and each month\n",
        "ranked_songs_df_month = exploded_df_month.withColumn(\"rank\", row_number().over(window_spec_month)) \\\n",
        "                             .filter(F.col(\"rank\") <= 5)\n",
        "\n",
        "# Join with the DataFrame containing song information to get the song name\n",
        "top_songs_with_names_df_month = ranked_songs_df_month.join(df_tracks, \"track_id\", \"left\")\n",
        "\n",
        "# Aggregate the counts of the top 5 songs for each user and each month\n",
        "top_songs_count_df_month = top_songs_with_names_df_month.groupBy(\"user_id\", month(\"session_window.start\").alias(\"month\")) \\\n",
        "                                             .agg(F.collect_list(\"name\").alias(\"top_songs\"),\n",
        "                                                  F.collect_list(\"count\").alias(\"count\"))\n",
        "\n",
        "# Show the result\n",
        "top_songs_count_df_month.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxAusHvpIxli",
        "outputId": "3a305b8a-78db-4170-e39f-dfc0ac513871"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "|user_id|month|top_songs                                                                                                        |count               |\n",
            "+-------+-----+-----------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "|0      |2    |[Dónde Estás, Amazing Grace, Amazing Grace, Dónde Estás, Dónde Estás]                                            |[9, 8, 8, 8, 8]     |\n",
            "|1      |2    |[The Humbling River, Twilight, For You, The Humbling River, Twilight]                                            |[11, 8, 8, 8, 7]    |\n",
            "|2      |2    |[Delirium, Us Against The World (Da Tweekaz Remix), Delirium, Sinto Falta Dela, 情歌]                            |[13, 8, 7, 6, 6]    |\n",
            "|3      |2    |[Escrito Nas Estrelas, Loving Strangers, Kamini, Rockin' Around the Christmas Tree, Heading Home]                |[13, 10, 10, 10, 10]|\n",
            "|4      |2    |[Chantilly Lace, White Lightnin', White Lightnin', Chantilly Lace, stay with me]                                 |[14, 14, 10, 10, 6] |\n",
            "|5      |2    |[Macarena, Necesitamos Mas Acción, La Candela, Não Vá Se Perder Por Aí, Avarandado]                              |[15, 15, 15, 15, 15]|\n",
            "|6      |2    |[Poovukkul, Ave Maria, Alberto Balsalm, Jogi, Showdown]                                                          |[13, 13, 10, 10, 10]|\n",
            "|7      |2    |[Contemplation, Filho Do Deus Vivo - Ao Vivo, Teu Amor Não Falha - Ao Vivo, Que Se Abram os Céus - Ao Vivo, Aria]|[18, 12, 12, 12, 10]|\n",
            "|8      |2    |[Tie, Tie, Bohemian Sunset, Drew Barrymore, Severek Ayrılanlar]                                                  |[19, 10, 8, 8, 8]   |\n",
            "|9      |2    |[Reykjavik, How Can You Mend A Broken Heart, Reykjavik, Lo Siento, Lo Siento]                                    |[14, 14, 11, 11, 9] |\n",
            "+-------+-----+-----------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_top_5_songs_current_month(user_id):\n",
        "  latest_row = get_user_latest_month(top_songs_count_df_month, user_id)\n",
        "\n",
        "  return latest_row[\"top_songs\"]"
      ],
      "metadata": {
        "id": "l5guAMjq_4qE"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Monthly listening time / count of songs"
      ],
      "metadata": {
        "id": "65tp0JMUAZiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, month, sum, count"
      ],
      "metadata": {
        "id": "gcMFsjTVAi--"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df_month = result_df.withColumn(\"track_id\", explode(F.map_keys(\"track_count_dict\"))) \\\n",
        "                             .withColumn(\"count\", explode(F.map_values(\"track_count_dict\")))\n",
        "\n",
        "# Define a window specification to partition by user_id and month\n",
        "window_spec_month = Window.partitionBy(\"user_id\", month(\"session_window.start\"))\n",
        "\n",
        "# Aggregate the listening time and number of songs for each user and month\n",
        "aggregated_df_month = exploded_df_month.groupBy(\"user_id\", month(\"session_window.start\").alias(\"month\")) \\\n",
        "                                       .agg(sum(\"session_duration\").alias(\"listening_time\"),\n",
        "                                            count(\"track_id\").alias(\"number_of_songs\"))\n",
        "\n",
        "# Show the result\n",
        "aggregated_df_month.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpqQk-2DAfFh",
        "outputId": "56d27ba7-5528-412f-eec2-b227886bb14a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------------+---------------+\n",
            "|user_id|month|listening_time|number_of_songs|\n",
            "+-------+-----+--------------+---------------+\n",
            "|      0|    2|        142371|            192|\n",
            "|      1|    2|        236656|            284|\n",
            "|      2|    2|        105531|            250|\n",
            "|      3|    2|         75711|             59|\n",
            "|      4|    2|        137931|            338|\n",
            "|      5|    2|        489995|            408|\n",
            "|      6|    2|        119323|            151|\n",
            "|      7|    2|        232783|            192|\n",
            "|      8|    2|        376129|            364|\n",
            "|      9|    2|        114491|            146|\n",
            "+-------+-----+--------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_listening_time_current_month(user_id):\n",
        "  latest_row = get_user_latest_month(aggregated_df_month, user_id)\n",
        "\n",
        "  return latest_row[\"listening_time\"]"
      ],
      "metadata": {
        "id": "Hdc0K_1XAkup"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_total_listens_current_month(user_id):\n",
        "  latest_row = get_user_latest_month(aggregated_df_month, user_id)\n",
        "\n",
        "  return latest_row[\"number_of_songs\"]"
      ],
      "metadata": {
        "id": "mQQ5FS7fAvBB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 Percentile Analysis"
      ],
      "metadata": {
        "id": "I6mFKXP3BTH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col, month, expr\n",
        "from pyspark.sql.window import Window\n",
        "from tdigest import TDigest"
      ],
      "metadata": {
        "id": "MahnxAFgBa50"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to compute the percentile using T-Digest\n",
        "def compute_percentile(counts):\n",
        "    tdigest = TDigest()\n",
        "    for count in counts:\n",
        "        tdigest.update(count)\n",
        "    return tdigest.percentile(0.99)  # Change the percentile as needed\n",
        "\n",
        "# UDF to apply the percentile function\n",
        "compute_percentile_udf = F.udf(compute_percentile)\n",
        "\n",
        "# Explode the track_count_dict map into separate rows for each track and its count\n",
        "exploded_df = result_df.withColumn(\"track_id\", explode(F.map_keys(\"track_count_dict\"))) \\\n",
        "                        .withColumn(\"count\", explode(F.map_values(\"track_count_dict\")))\n",
        "\n",
        "window_spec = Window.partitionBy(\"user_id\", month(\"session_window.start\")).orderBy(col(\"count\").desc())\n",
        "\n",
        "ranked_songs_df = exploded_df.withColumn(\"rank\", F.row_number().over(window_spec))\n",
        "\n",
        "top_songs_df = ranked_songs_df.filter(col(\"rank\") == 1)\n",
        "\n",
        "top_songs_count_df = top_songs_df.groupBy(\"user_id\", month(\"session_window.start\").alias(\"month\"), \"track_id\") \\\n",
        "                                  .agg(F.sum(\"count\").alias(\"count\"))\n",
        "\n",
        "top_songs_with_names_df = top_songs_count_df.join(df_tracks, \"track_id\", \"left\") \\\n",
        "                                             .withColumnRenamed(\"name\", \"song_name\") \\\n",
        "                                             .select(\"user_id\", \"month\", \"song_name\", \"count\")\n",
        "\n",
        "percentile_df = top_songs_with_names_df.groupBy(\"user_id\", \"month\", \"song_name\").agg(F.collect_list(\"count\").alias(\"counts\")) \\\n",
        "                                  .withColumn(\"percentile\", compute_percentile_udf(\"counts\").cast(\"double\"))\n",
        "\n",
        "results_df_month = percentile_df.withColumn(\"top_percentile\",\n",
        "                                     expr(\"transform(counts, x -> if(x = percentile, 'Top 1%', 'Not Top 5%'))\"))\n",
        "\n",
        "# Show the result with all required columns\n",
        "results_df_month.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La2VGNNOBXGU",
        "outputId": "08ce44c1-326d-4dc2-8fca-ff368eb275b8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------------------+------+----------+--------------+\n",
            "|user_id|month|song_name           |counts|percentile|top_percentile|\n",
            "+-------+-----+--------------------+------+----------+--------------+\n",
            "|0      |2    |Dónde Estás         |[9]   |9.0       |[Top 1%]      |\n",
            "|1      |2    |The Humbling River  |[11]  |11.0      |[Top 1%]      |\n",
            "|2      |2    |Delirium            |[13]  |13.0      |[Top 1%]      |\n",
            "|3      |2    |Escrito Nas Estrelas|[13]  |13.0      |[Top 1%]      |\n",
            "|4      |2    |Chantilly Lace      |[14]  |14.0      |[Top 1%]      |\n",
            "|5      |2    |Macarena            |[15]  |15.0      |[Top 1%]      |\n",
            "|6      |2    |Poovukkul           |[13]  |13.0      |[Top 1%]      |\n",
            "|7      |2    |Contemplation       |[18]  |18.0      |[Top 1%]      |\n",
            "|8      |2    |Tie                 |[19]  |19.0      |[Top 1%]      |\n",
            "|9      |2    |Reykjavik           |[14]  |14.0      |[Top 1%]      |\n",
            "+-------+-----+--------------------+------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_top_song_percentile_current_month(user_id):\n",
        "  latest_row = get_user_latest_month(results_df_month, user_id)\n",
        "\n",
        "  return latest_row[\"song_name\"], latest_row[\"top_percentile\"][0]"
      ],
      "metadata": {
        "id": "DZgBm2S4BdKG"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.4 User engagement"
      ],
      "metadata": {
        "id": "ESdtsht1EZad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month, col\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "JJPMY2liR863"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the user_id\n",
        "user_id = 1"
      ],
      "metadata": {
        "id": "QD51vLnTR_YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter result_df for the specific user_id\n",
        "user_result_df = result_df.filter(col(\"user_id\") == user_id)\n",
        "\n",
        "# Calculate session duration for the specific user\n",
        "user_result_df = user_result_df.withColumn(\"session_duration_minutes\", (col(\"session_window.end\").cast(\"long\") - col(\"session_window.start\").cast(\"long\")) / 60)\n",
        "\n",
        "# Extract month from session window for the specific user\n",
        "user_result_df = user_result_df.withColumn(\"month\", month(\"session_window.start\"))\n",
        "\n",
        "# Group by month and calculate average session duration for the specific user\n",
        "average_session_duration_per_month_user = user_result_df.groupby(\"month\").agg({\"session_duration_minutes\": \"avg\"})\n",
        "\n",
        "# Order by month for the specific user\n",
        "average_session_duration_per_month_user = average_session_duration_per_month_user.orderBy(\"month\")\n",
        "\n",
        "# Convert to Pandas DataFrame for plotting for the specific user\n",
        "average_session_duration_per_month_pd_user = average_session_duration_per_month_user.toPandas()\n",
        "\n",
        "# Group by month and calculate average session duration for all users\n",
        "average_session_duration_per_month_all = result_df.groupby(month(\"session_window.start\").alias(\"month\")).agg({\"session_duration_minutes\": \"avg\"})\n",
        "\n",
        "# Order by month for all users\n",
        "average_session_duration_per_month_all = average_session_duration_per_month_all.orderBy(\"month\")\n",
        "\n",
        "# Convert to Pandas DataFrame for plotting for all users\n",
        "average_session_duration_per_month_pd_all = average_session_duration_per_month_all.toPandas()\n",
        "\n",
        "# Plot both lines in one plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot average session duration per month for all users\n",
        "plt.plot(average_session_duration_per_month_pd_all[\"month\"], average_session_duration_per_month_pd_all[\"avg(session_duration_minutes)\"], marker='o', label=\"All Users\")\n",
        "\n",
        "# Plot average session duration per month for the specific user\n",
        "plt.plot(average_session_duration_per_month_pd_user[\"month\"], average_session_duration_per_month_pd_user[\"avg(session_duration_minutes)\"], marker='s', label=f\"User {user_id}\")\n",
        "\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Average Session Duration (minutes)\")\n",
        "plt.title(\"Average Session Duration Per Month\")\n",
        "plt.xticks(average_session_duration_per_month_pd_all[\"month\"])\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Nc0Twcc7EdmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. SONG RECOMMENDATIONS"
      ],
      "metadata": {
        "id": "zBHj6x3gEhDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Avro file and rename the column track_id to song_id\n",
        "df_songs = spark.read.format(\"avro\").load(\"/content/drive/MyDrive/stream/data/tracks.avro\") \\\n",
        "    .withColumnRenamed(\"track_id\", \"song_id\")"
      ],
      "metadata": {
        "id": "LevT68ewElo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary functions\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Join the streaming DataFrame df with df_tracks on the track_id column\n",
        "joined_df = df.join(df_songs, df[\"track_id\"] == df_songs[\"song_id\"], \"inner\")"
      ],
      "metadata": {
        "id": "0-nrFAWSTAr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_merged = 'merged_df'\n",
        "query = joined_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(query_merged) \\\n",
        "    .start()\n",
        "df_merged = spark.sql(\"SELECT * FROM merged_df\")"
      ],
      "metadata": {
        "id": "5SNMXMdKTCmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, rank\n",
        "from pyspark.sql import Window\n",
        "\n",
        "\n",
        "# Group by user_id and song_id, count the occurrences of each song for each user\n",
        "user_song_counts = df_merged.groupBy(\"user_id\", \"song_id\").count()\n",
        "\n",
        "# Assign a rank to each song within each user group based on the count, ordered by count in descending order\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"count\").desc())\n",
        "user_song_rank = user_song_counts.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "# Filter to keep only the rows where the rank is 1, indicating the most played song for each user\n",
        "most_played_per_user = user_song_rank.filter(col(\"rank\") == 1).drop(\"rank\")\n",
        "\n",
        "# Join most_played_per_user with df_tracks on song_id\n",
        "most_played_with_characteristics = most_played_per_user.join(df_songs, \"song_id\", \"left\")\n",
        "\n",
        "# Select the desired columns from df_tracks and keep the user_id and song_id\n",
        "selected_columns = [\n",
        "    \"user_id\", \"song_id\", \"duration\", \"artist\", \"name\", \"popularity\", \"release_date\",\n",
        "    \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\",\n",
        "    \"speechiness\", \"acousticness\", \"instrumentalness\", \"liveness\",\n",
        "    \"valence\", \"tempo\", \"time_signature\", \"album_name\", \"track_genre\"\n",
        "]\n",
        "\n",
        "# Show the result\n",
        "best_song = most_played_with_characteristics.select(selected_columns)\n",
        "best_song.show()"
      ],
      "metadata": {
        "id": "95dvOanJTDFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_songs_pd = df_songs.toPandas()\n",
        "best_song_pd = best_song.toPandas()"
      ],
      "metadata": {
        "id": "GVPC9W2BTDlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_top_similar_songs(song_id, df_songs_pd):\n",
        "    # Define vector_columns\n",
        "    vector_columns = [\"duration\", \"popularity\", \"danceability\", \"energy\", \"key\",\n",
        "                      \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\",\n",
        "                      \"liveness\", \"valence\", \"tempo\", \"time_signature\"]\n",
        "\n",
        "    # Extract the features of the target song\n",
        "    target_features = df_songs_pd.loc[df_songs_pd['song_id'] == song_id, vector_columns].values\n",
        "    if len(target_features) == 0:\n",
        "        return []  # Return an empty list if the song_id is not found\n",
        "\n",
        "    target_features = target_features[0]\n",
        "\n",
        "    # Calculate cosine similarity between the target song and all other songs\n",
        "    other_features = df_songs_pd.loc[df_songs_pd['song_id'] != song_id, vector_columns].values\n",
        "    dot_products = np.dot(other_features, target_features)\n",
        "    target_norm = np.linalg.norm(target_features)\n",
        "    other_norms = np.linalg.norm(other_features, axis=1)\n",
        "    similarities = dot_products / (target_norm * other_norms)\n",
        "\n",
        "    # Get indices of top 5 most similar songs\n",
        "    top_indices = np.argsort(similarities)[::-1][:5]\n",
        "\n",
        "    # Get the names of the top similar songs\n",
        "    top_similar_songs = df_songs_pd.loc[df_songs_pd.index[top_indices], 'name'].tolist()\n",
        "\n",
        "    return top_similar_songs\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "best_song_pd['recommendations'] = best_song_pd['song_id'].apply(lambda x: get_top_similar_songs(x, df_songs_pd))\n",
        "\n",
        "# Display the DataFrame with recommendations\n",
        "print(best_song_pd)"
      ],
      "metadata": {
        "id": "AyWOVy0dTDuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-storage-queue"
      ],
      "metadata": {
        "id": "PcdMvMMBTD07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "username= 'yassine02' # this can be any string identifier, for example your email username\n",
        "account_name= 'ysnspotify' # the name of the Azure Storage Account you want to use\n",
        "account_key= 'ewdKctJnob2ion3ihNKmTCqQzqnUv2/tTG4xcPpYqYEoK7P7IQJPxrnorYBa2QKQi7k0nEsoPv1Z+AStyDW4qA=='\n",
        "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
        "\n",
        "\n",
        "connection_string"
      ],
      "metadata": {
        "id": "vH0h8bkkTD9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_user_queue(user_id):\n",
        "    queue_name = f\"{username}-{user_id}-queue\"\n",
        "    try:\n",
        "        queue_service_client.create_queue(queue_name)\n",
        "        print(f\"Queue '{queue_name}' created successfully for user {user_id}.\")\n",
        "    except ResourceExistsError:\n",
        "        print(f\"Queue '{queue_name}' already exists for user {user_id}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while creating queue for user {user_id}: {e}\")\n",
        "\n",
        "# Function to send a message to the queue without retries\n",
        "def send_to_queue(user_id, recommendation_rank, recommendation_name):\n",
        "    queue_name = f\"{username}-{user_id}-queue\"\n",
        "    queue_client = queue_service_client.get_queue_client(queue_name)\n",
        "    message = f\"User ID: {user_id}, Recommendation Rank: {recommendation_rank}, Recommendation Name: {recommendation_name}\"\n",
        "\n",
        "    try:\n",
        "        queue_client.send_message(message)\n",
        "        print(f\"Recommendation for user {user_id} (Rank: {recommendation_rank}) sent to queue.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while sending message to queue {queue_name}: {e}\")\n",
        "\n",
        "# Iterate over each user and send recommendations to their respective queues\n",
        "for user_id, row in best_song_pd.iterrows():\n",
        "    create_user_queue(user_id)  # Create queue for the user\n",
        "    recommendations = row['recommendations']\n",
        "    for rank, recommendation in enumerate(recommendations, start=1):\n",
        "        send_to_queue(user_id, rank, recommendation)"
      ],
      "metadata": {
        "id": "48BLnsdITEFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_alert(user_id, recommendations):\n",
        "    if len(recommendations) < 5:\n",
        "        alert_record = {'user_id': user_id, 'message': f'Recommendations for user {user_id} are less than 5'}\n",
        "        send_alert(alert_record)\n",
        "    # You can add more conditions here based on your requirements, such as checking if recommendations are sent to the wrong user.\n",
        "    # For example, you could compare the user_id in the recommendation with the actual user_id.\n",
        "\n",
        "# Iterate over each user and send recommendations to their respective queues\n",
        "for user_id, row in best_song_pd.iterrows():\n",
        "    create_user_queue(user_id)  # Create queue for the user\n",
        "    recommendations = row['recommendations']\n",
        "    for rank, recommendation in enumerate(recommendations, start=1):\n",
        "        send_to_queue_with_retry(user_id, rank, recommendation)\n",
        "\n",
        "    # Check for alerts\n",
        "    check_and_alert(user_id, recommendations)"
      ],
      "metadata": {
        "id": "juIOQz9uTEO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FLASK APP"
      ],
      "metadata": {
        "id": "sxvSn5s539pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kill -9 \"$(pgrep ngrok)\""
      ],
      "metadata": {
        "id": "5g89Vd839BfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b1c9d9-fab4-4b2e-846a-8f4e3d98ce84"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: kill: `': not a pid or valid job spec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlbEG4EEj6Lb",
        "outputId": "a7a8b65d-ac41-4cd5-fc26-228ebceb7eee"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('drive/MyDrive/stream/src')"
      ],
      "metadata": {
        "id": "5gavNxFoF6Fj"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_familiarity_exploration_score(total_listens, listens_with_unique_artist):\n",
        "    return (total_listens - listens_with_unique_artist) / total_listens\n",
        "\n",
        "def get_timelessness_newness_score(average_user_release_date, average_dataset_release_date):\n",
        "    return average_user_release_date / average_dataset_release_date\n",
        "\n",
        "def get_loyalty_variety_score(loyalty_counter, total_listens):\n",
        "    return loyalty_counter / total_listens\n",
        "\n",
        "def get_commonality_uniqueness_score(mainstream_counter, total_listens):\n",
        "    return mainstream_counter / total_listens\n",
        "\n",
        "\n",
        "def get_personality_traits(user_id):\n",
        "    traits = \"\"\n",
        "    traits_list = []\n",
        "\n",
        "    total_listens = get_user_total_listens(user_id)\n",
        "    listens_with_unique_artist = get_user_listens_unique_artist(user_id)\n",
        "    average_user_release_date = get_user_average_release_date(user_id)\n",
        "    average_dataset_release_date = get_average_release_date()\n",
        "    loyalty_counter = get_user_loyalty_counter(user_id)\n",
        "    mainstream_counter = get_user_mainstream_counter(user_id)\n",
        "\n",
        "    try:\n",
        "      fe_score = get_familiarity_exploration_score(total_listens, listens_with_unique_artist)\n",
        "    except ZeroDivisionError:\n",
        "      fe_score = 1\n",
        "    if fe_score > 0.5:\n",
        "        traits += \"F\"\n",
        "        traits_list.append(\"Familiarity\")\n",
        "    else:\n",
        "        traits += \"E\"\n",
        "        traits_list.append(\"Exploration\")\n",
        "    try:\n",
        "      te_score = get_timelessness_newness_score(average_user_release_date, average_dataset_release_date)\n",
        "    except ZeroDivisionError:\n",
        "      te_score = 1\n",
        "    if te_score >= 1:\n",
        "        traits += \"N\"\n",
        "        traits_list.append(\"Newness\")\n",
        "    else:\n",
        "        traits += \"T\"\n",
        "        traits_list.append(\"Timelessness\")\n",
        "    try:\n",
        "      lv_score = get_loyalty_variety_score(loyalty_counter, total_listens)\n",
        "    except ZeroDivisionError:\n",
        "      lv_score = 1\n",
        "    if lv_score > 0.5:\n",
        "        traits += \"L\"\n",
        "        traits_list.append(\"Loyalty\")\n",
        "    else:\n",
        "        traits += \"V\"\n",
        "        traits_list.append(\"Variety\")\n",
        "    try:\n",
        "      cu_score = get_commonality_uniqueness_score(mainstream_counter, total_listens)\n",
        "    except ZeroDivisionError:\n",
        "      cu_score = 1\n",
        "    if cu_score > 0.5:\n",
        "        traits += \"C\"\n",
        "        traits_list.append(\"Commonality\")\n",
        "    else:\n",
        "        traits += \"U\"\n",
        "        traits_list.append(\"Uniqueness\")\n",
        "\n",
        "    return traits, traits_list\n",
        "\n",
        "\n",
        "def get_personality_type(user_id):\n",
        "    personality_map = {\n",
        "        \"ENVC\": 'The Early Adopter',\n",
        "        \"ENLU\": 'The Nomad',\n",
        "        \"FNVU\": 'The Specialist',\n",
        "        \"FNLC\": 'The Enthusiast',\n",
        "        \"FTLC\": 'The Connoisseur',\n",
        "        \"FTVU\": 'The Deep Diver',\n",
        "        \"FNVC\": 'The Fanclubber',\n",
        "        \"ETLC\": 'The Top Charter',\n",
        "        \"FTLU\": 'The Replayer',\n",
        "        \"FTVC\": 'The Jukeboxer',\n",
        "        \"ENLC\": \"The Voyager\",\n",
        "        \"FNLU\": \"The Devotee\",\n",
        "        \"ETLU\": \"The Maverick\",\n",
        "        \"ETVU\": \"The Time Traveler\",\n",
        "        \"ETVC\": \"The Musicologist\",\n",
        "        \"ENVU\": \"The Adventurer\"\n",
        "    }\n",
        "    traits, _ = get_personality_traits(user_id)\n",
        "\n",
        "    return personality_map[traits]"
      ],
      "metadata": {
        "id": "kyBpD2RjKhtY"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import threading\n",
        "\n",
        "from flask import Flask, render_template, request, redirect, url_for\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "\n",
        "conf.get_default().auth_token = \"2f3FZdCmboHeGLduyiPd3PuvG5h_2kiQwA9e3XyG76J1PAiLj\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}/\\\"\".format(public_url, 5000))\n",
        "\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "  if request.method == 'POST':\n",
        "    user_input = request.form['user_input']\n",
        "    return redirect(url_for('wrapped', user_id=user_input))\n",
        "\n",
        "  context = {\n",
        "      \"top_songs_week\": get_top_5_streamed_songs_current_week(),\n",
        "      \"top_songs_month\": get_top_5_streamed_songs_current_month(),\n",
        "      \"top_artists_week\": get_top_5_streamed_artists_current_week(),\n",
        "      \"top_artists_month\": get_top_5_streamed_artists_current_month(),\n",
        "  }\n",
        "\n",
        "  return render_template('index.html', **context)\n",
        "\n",
        "\n",
        "@app.route('/wrapped/<user_id>')\n",
        "def wrapped(user_id):\n",
        "    _, traits_list = get_personality_traits(user_id)\n",
        "    top_song_week, percentile_week = get_user_top_song_percentile_current_week(user_id)\n",
        "    top_song_month, percentile_month = get_user_top_song_percentile_current_month(user_id)\n",
        "\n",
        "    context = {\n",
        "       \"user_id\": user_id,\n",
        "       \"personality_type\": get_personality_type(user_id),\n",
        "       \"personality_traits\": traits_list,\n",
        "       \"total_listens_week\": get_user_total_listens_current_week(user_id),\n",
        "       \"total_listening_time_week\": get_user_listening_time_current_week(user_id) / 3600,\n",
        "       \"total_listens_month\": get_user_total_listens_current_month(user_id),\n",
        "       \"total_listening_time_month\": get_user_listening_time_current_month(user_id) / 3600,\n",
        "       \"top_song_week\": top_song_week,\n",
        "       \"percentile_week\": percentile_week,\n",
        "       \"top_song_month\": top_song_month,\n",
        "       \"percentile_month\": percentile_month,\n",
        "       \"top_5_week\": get_user_top_5_songs_current_week(user_id),\n",
        "       \"top_5_month\": get_user_top_5_songs_current_month(user_id)\n",
        "    }\n",
        "\n",
        "    return render_template('wrapped.html', **context)\n",
        "\n",
        "threading.Thread(target=app.run, kwargs={\"debug\": True,\"use_reloader\": False}).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3S1e6LOZagv",
        "outputId": "bdf0d03e-de05-4dc9-fd10-3388411cdf19"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-04-16T02:35:17+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://5e40-34-71-59-192.ngrok-free.app\" -> \"http://127.0.0.1:5000/\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        }
      ]
    }
  ]
}